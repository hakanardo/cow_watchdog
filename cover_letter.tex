\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\begin{document}
\section{Theoretical foundation}

Övergripande:
The main hyposthesis in this works involves investigating the possibility to extract cow behaviour from video data.
This hypothesis is confirmed by experimental valibdation with a new proposed method for detection and validation of cow behaviour.
These result are connected to "cow behaviour" model(?)..etc

Teknisk formulering exempel ("nytt" är arkitektur som är nytt ramverk):
The general theoretical technical detection parts are related to the foundations regarding machine learning principles where we introduce softmax on cow parts, utilizing softmax operation, followed by a rotated rectanle network.
plus texten innan (kanske kortare)


Animal behaviour and welfare can be studied/assessed by looking at different interactions occurring between the animals. Video recordings of a scene of interest are often made and then watched/evaluated by experts. Some recent studies on cow behaviour in the dairy barn environment were based on different GPS or wireless sensor network (WSN) solutions \cite{Nadimietal2012}. These solutions allow scientists to see spatial distribution of animals and to measure varying levels of activity \cite{Nadimietal2012}. However, the number and complexity of behaviours that could be monitored with the position-based approach is limited.
 Therefore, new methods based on video surveillance and image analysis, which could extend the number of parameters for studying, are of great importance. This is achieved using a cow detector that detects cows in the scene and represents them with rotated bounding boxes. That means the number of cows could be counted and distances between the cows can be measured. Rotated bounding boxes allow much more precise distance measures between diagonally oriented cows as compared to more classical non-rotated bounding boxes due to the elongated aspect ratio of the cows. 
 Most previous work on detecting cows by using video cameras have been focused on monitoring areas where the orientation of the cows was known due to physical limitations imposed by the surroundings. 
 Two examples of this are the Viola-Jones based detector of Arcidiacono et al. \cite{Arcidiacono2012} for detecting cows at the feed barrier and the work of Martinez-Ortiz1 et al. 
 \cite{martinez2013video} to detect and track cow heads in narrow entrance corridors. 
 There are also approaches that rely on more advanced sensors, 
 such as Abdul et. al \cite{abdul2016locomotion} that uses a depth camera.

General purpose object detection frameworks such as YOLO9000 \cite{FIXME} and SSD \cite{FXIME} have very good performance.
 They do however focus on detecting objects of varying size and aspect ratio but with fix orientation. 
 In the scenario considered in this paper, 
 the size and aspect ratio is fixed and known while orientation (rotation) 
 varies and have to be estimated.

Animalisk (nytt är baserat på behaviour - och vi validerar metod):
Något om animal och beteende, specifik om det går eller mer generellt om det inte går (referenser till någon model här också kanske).
- texten innan (kanske kortare)


\section{Main contributions}
A novel approach towards cow interaction involving a two step CNN for rotated cow detection with a following interaction analysis.

\section{Closest work}
The the closest work and current state of the art for detecting cows freely moving around was presented by Porto et al. \cite{porto2015automatic}. They use a Viola-Jones based detector and need 
6 cameras to cover a $15.4 \times 3.8$ m area and detect cows in 3 different orientations: vertical, horizontal and diagonal with a hit rate of 90\%. They do not use separate detectors for the two different diagonals which mean that producing rotated bounding box from their results would not be straight forward. In work presented in this paper, only 3 cameras were needed to cover a $18 \times 6$ m area and detect cows in 32 different orientations with a hit rate of 97\%. Note that the hit rates are from different datasets and that the dataset used in this article have larger variations in viewpoints due to the use of fewer cameras to cover the same area.

\section{State of the art}
Section "1.1 Related work" contains a review of the state of the art and includes comparative evaluation of your work against the closes work mentioned above.

\section{Originality}
This submission is our original work. An earlier version was presented at the Visual observation and analysis of Vertebrate And Insect Behavior 2016 workshop  in conjunction with 23rd International Conference on Pattern Recognition (ICPR 2016). A copy of that version is included in the submission as VAIB2016.pdf Sections XXX are new and XXX revised.

\bibliographystyle{plain}
{\parindent0pt
\parskip8pt
\bibliography{main}
}


\end{document}