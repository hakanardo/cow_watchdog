\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Håkan Ardö, Oleksiy Guzhva, Mikael Nilsson}
\title{Cow interaction watch dog}
\begin{document}
\maketitle

\begin{abstract}
Cows are interacting...
\end{abstract}

\section{Introduction}

XXX scientists are interested in studying the social interactions between cows in diary farms. Typically that is performed by defining a set of interaction events such as head butting, body pushing, social licking, mounting, ... Then an expert studies an area of interest for large amount mount of time and counts them number of each event. Some of these events are quite rare, which means that a lot of expert time have to be spent in looking at uninteresting events.

In this paper the goal is to take the first step towards an automated system. The study area is filmed using video cameras. Then an automated watchdog will remove a lot of the uninteresting parts of that video. The remaining video will still have to be studied by experts, but the time spent looking at uninteresting video will be significantly reduced.

The pilot study used to develop this watchdog was made in a dairy barn in the south of Sweden with 252 Swedish Holstein cows. They were milked by four automated milking robots, which had a common waiting area (6x18 meters). This waiting area is an open space which several cows ready for milking are allowed to enter. They will then interact with each other in order to decide who are allowed to enter each of the milking robots first.

Video recordings were made using three Axis M3006-V cameras with a wide angle of 134 degrees and placed at 3.6 meter height pointing straight down to optimize overview over the study area. There is a significant overlap between the camera images in order to not miss events taking place at the border between the cameras. In total 2315 hours (1 month) of 800x600 video in 16 fps was collected.

The cameras was calibrated to compensate for lens distortion and rectified. Although the cameras was physically mounted to point fairly straight down, they was still slightly tiled. This tilting was synthetically removed during this rectification. The end result of this calibration is video images where the cows have the same size regardless of where in the image they appear. Also the scan-lines of the three different cameras becomes aligned which allows them to be stitched together to form an overview of the entire waiting area.

Finally a CNN was trained to detect the cows in the images, and statistics about how many cows and their distances/relation to each other was extracted. Using these statistics the XXX scientists can form queries to select time intervals of interest to watch, such as "show me video clips involving at least two cows with the neck of one cow closer than one meter to the body of the other".

\section{CNN cow detector}

A random subset of the full recording consisting of 1722 images was manually annotated. This subset contained in total 6399 cows. Each cow was annotated with seven landmark points: head, left and right shoulder, front middle, left and right hip and back middle. In addition to that one additional landmark "cow center" was defined as the mean of front middle and back middle. This data was then used to train a CNN detector.

The detector was split into two step. The first step is a fully convolutional CNN that detects the landmarks in the image. Currently only four of the landmarks was used to speed up the experiments, but extending to use all 8 is straight forward. The architecture of this network is a fully convolutional version of VGG \cite{FIXME} with batch normalisation \cite{DBLP:journals/corr/IoffeS15} added after each convolution step. Details are shown in Table \ref{tab:cownet}.

The second step is another CNN that works with the probability map produced by the first as input and tries to detect the cows and their orientations. The full circle is divided into 32 equally spaced orientations which generates 32 different oriented cow classes. In addition to that there is the no cow class, which makes the total number of classes of this CNN 33. The input probabilities turned into log likelihoods as it makes more sense when summing them together. Then the network consists of a single $ 13 \times 13 $ convolutional layer. Details are shown in Table \ref{tab:cowdirnet}.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline 
\textbf{Layer type} & \textbf{Size} & \textbf{Channels} \\ 
\hline 

Conv + BNorm + Relu & 3x3 & 32 \\ 
MaxPool(stride=2) & 2x2 &  \\ 
\hline 

Conv + BNorm + Relu & 3x3 & 64 \\ 
MaxPool(stride=2) & 2x2 &  \\ 
\hline 

Conv + BNorm + Relu & 3x3 & 128 \\ 
Conv + BNorm + Relu & 3x3 & 128 \\ 
MaxPool(stride=2) & 2x2 &  \\ 
\hline 

Conv + BNorm + Relu & 3x3 & 256 \\ 
Conv + BNorm + Relu & 3x3 & 256 \\ 
MaxPool(stride=2) & 2x2 &  \\ 
\hline 

Conv + BNorm + Relu & 3x3 & 512 \\ 
Conv + BNorm + Relu & 3x3 & 512 \\ 
MaxPool(stride=2) & 2x2 &  \\ 
\hline 

Conv + BNorm + Relu & 1x1 & 1024 \\ 
Conv + BNorm + Relu & 1x1 & 1024 \\ 
Conv + BNorm + Relu & 1x1 & 5 \\ 
Softmax & & \\
\hline 

\end{tabular} 
\end{center}
\caption{CNN architecture used to detect different landmarks of the cows. The input is an image of any size with 3 rgb channels scaled to the range $ 0\cdots1 $. The output is probability map segmenting the entire image into 5 classes: Ground, Cow front middle, Cow center, Cow back middle and Cow head.}
\label{tab:cownet}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline 
\textbf{Layer type} & \textbf{Size} & \textbf{Channels} \\ 
\hline 

MaxPool(stride=1) & 3x3 &  \\ 
Log & & \\
Conv + BNorm + Relu & 13x13 & 33 \\ 
Softmax & & \\
\hline 
\end{tabular}
\end{center}
\caption{CNN architecture use to detect the cows and their orientation. The input is the 5 channel probability map from the landmark detector with the last MaxPool removed to increase resolution. The output is a probability map that segments the image into either background or cow in one of 32 different orientations.}
\label{tab:cowdirnet}
\end{table}

XXX: Landmarks to trainingdata

The weights of the convolutions are initiated using random samples draw from a Gaussian 
distribution truncated at $2\sigma$, with standard deviation $\sigma=\sqrt{\frac{2}{n}}$, 
where $n$ is the number of inputs. The networks are regularized with weight decay of 
$0.0001$ and optimized using stochastic gradient descent with $0.9$ momentum. The 
learning rate is initiated to $1.0$ and reduced by a factor $10$ each time the validation 
error flattens. The landmark CNN uses only valid outputs from the convolutional and maxpool 
layers while the cow detector keeps the same resolution to also detect cows that are 
slightly outside the image.

\end{document}